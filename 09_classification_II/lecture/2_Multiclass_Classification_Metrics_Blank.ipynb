{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83aeb84d",
   "metadata": {},
   "source": [
    "# Multiclass Classification Metrics\n",
    "\n",
    "Let's expand our model diagnostic toolbox to account for classification problems with more than one possible outcome.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Discuss how to assess multiclass classifiers,\n",
    "- Expand the confusion matrix to account for more than two classes and\n",
    "- Introduce cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5413a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a dark background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f732f48c",
   "metadata": {},
   "source": [
    "## Making multiclass into binary\n",
    "\n",
    "The first way you may evaluate a multiclass model is to turn it into a binary model.\n",
    "\n",
    "Sometimes there may be a couple of classes that you are most interested in, in which case you can just focus on optimizing the performance of binary classifier metrics on those classes.\n",
    "\n",
    "For example, the Cleveland Heart Disease data set, <a href=\"https://archive.ics.uci.edu/ml/datasets/heart+disease\">https://archive.ics.uci.edu/ml/datasets/heart+disease</a>, provides four possible heart disease outcomes, but perhaps we are most interested in the most serious heart disease classification or the case where someone does not have heart disease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6af8b4",
   "metadata": {},
   "source": [
    "## Multiclass confusion matrix\n",
    "\n",
    "Whether we are interested in a couple of classes or all of the classes we can still gather useful information from the confusion matrix. The confusion matrix we introduced earlier naturally extends to more than two possible classes.\n",
    "\n",
    "<img src=\"lecture_9_assets/conf_mat_multi.png\" width=\"70%\"></img>\n",
    "\n",
    "However, in this setting we lose our ability to interpret things like true positive rate, false positve rate, etc.\n",
    "\n",
    "Let's see this in action in `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311550c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdc0692",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris(as_frame=True)\n",
    "\n",
    "X = iris['data']\n",
    "X = X.rename(columns={'sepal length (cm)':'sepal_length',\n",
    "                         'sepal width (cm)':'sepal_width',\n",
    "                         'petal length (cm)':'petal_length',\n",
    "                         'petal width (cm)':'petal_width'})\n",
    "y = iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84909302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e011c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X.copy(), y,\n",
    "                                                       shuffle=True,\n",
    "                                                       random_state=413,\n",
    "                                                       test_size=.2,\n",
    "                                                       stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d5e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.scatter(X_train.loc[y_train==0].petal_width, \n",
    "                X_train.loc[y_train==0].petal_length,\n",
    "                s = 60,\n",
    "                label='$y=0$')\n",
    "plt.scatter(X_train.loc[y_train==1].petal_width, \n",
    "                X_train.loc[y_train==1].petal_length,\n",
    "                marker = 'v',\n",
    "                s = 60,\n",
    "                label='$y=1$')\n",
    "plt.scatter(X_train.loc[y_train==2].petal_width, \n",
    "                X_train.loc[y_train==2].petal_length,\n",
    "                marker = 'x',\n",
    "                s = 60,\n",
    "                label='$y=2$')\n",
    "\n",
    "plt.xlabel(\"Petal Width (cm)\", fontsize=12)\n",
    "plt.ylabel(\"Petal Length (cm)\", fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e2662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e93fe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make LDA object\n",
    "LDA = LinearDiscriminantAnalysis()\n",
    "\n",
    "## Fit the model\n",
    "LDA.fit(X_train[['petal_width', 'petal_length']], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd73844",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b564f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b624da97",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(conf_mat,\n",
    "                 columns = ['Predicted 0', 'Predicted 1', 'Predicted 2'],\n",
    "                 index = ['Actual 0', 'Actual 1', 'Actual 2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb0015d",
   "metadata": {},
   "source": [
    "## Cross-entropy (aka logistic Loss, log-loss, or softmax loss)\n",
    "\n",
    "Assume that we have a probabilistic classifier with parameters $\\theta$.  \n",
    "\n",
    "For each $\\theta$, the classifier takes a feature vector $x$ as input and returns the vector $P_\\theta( y | x) = [P(y = 1 | x), P(y = 2 | x), \\dots P(y = C | x)]$ as output.  For instance in a 3 class problem we might get the output $[0.2, 0.7, 0.1]$ to indicate the probability of each of the three classes. In sklearn this will be the output of the `.predict_proba` method applied to a single row matrix.\n",
    "\n",
    "To match this convention, let's also output the actual class in the same format:  $[0,0,1]$ for example.  If you had originally had other some other convention (for example outputting the index $2$, or a name like 'iris-virginica'), the `pd.get_dummies` method will convert to the format we are talking about here.\n",
    "\n",
    "If we have $n$ independent observations $(x_i, y_i)$, then the likelihood function is\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\prod_{i=1}^n y_i \\cdot P_\\theta( y | \\vec{x}_i)\n",
    "$$\n",
    "\n",
    "Note that if our classifier is predicting a literal $0$ for a class probability, and a training example actual has that class, then we get a $0$ likelihood.  To avoid this, most probabilistic classifiers are constructed in such a way that $0$ probabilities are impossible to output. For instance, in logistic regression the sigmoid function has range $(0,1)$, so a class probability of $0$ or $1$ is never returned.\n",
    "\n",
    "Instead of maximizing the likelihood, we can minimize the negative log likelihood:\n",
    "\n",
    "$$\n",
    "\\operatorname{NLL}(\\theta) = -\\sum_{i=1}^n \\log( y_i \\cdot P_\\theta( y | \\vec{x}_i))\n",
    "$$\n",
    "\n",
    "This is a little easier to vectorize if we rewrite it as \n",
    "\n",
    "$$\n",
    "\\operatorname{NLL}(\\theta) = -\\sum_{i=1}^n y_i \\cdot \\log( P_\\theta( y | \\vec{x}_i))\n",
    "$$\n",
    "\n",
    "where we are taking the logarithm of each component of $P_\\theta( y | \\vec{x}_i)$.  This is okay since none of these components are $0$ and since only one term ultimately contributes to the sum.\n",
    "\n",
    "**This is the \"cross entropy\" of the classifier.**\n",
    "\n",
    "Let's give one hand computable example.\n",
    "\n",
    "<center>\n",
    "\n",
    "| Actual Class | Predicted Probs |\n",
    "|--------------|-----------------|\n",
    "| $[1,0,0]$    | $[0.8,0.1,0.1]$ |\n",
    "| $[0,0,1]$    | $[0.1,0.3,0.6]$ |\n",
    "| $[0,1,0]$    | $[0.09,0.01,0.9]$ |\n",
    "\n",
    "</center>\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\operatorname{CrossEntropy} \n",
    "&= - (\\log(0.8) + \\log(0.6) + \\log(0.01))\\\\\n",
    "&= - (-0.22 + -0.51 + -4.60)\\\\\n",
    "&= 0.22 + 0.51 + 4.60\\\\\n",
    "&= 5.34\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Notice that cross-entropy \"rewards\" the classifier with when it is confidently correct (like the first observation), and \"punishes\" the classifier when it is confidently incorrect (like the last example).\n",
    "\n",
    "Terry Tao proposes using cross-entropy as a way to score multiple choice examinations where the student is allowed to give a probability for each choice in [this blog post](https://terrytao.wordpress.com/2016/06/01/how-to-assign-partial-credit-on-an-exam-of-true-false-questions/comment-page-1/).  The derivation of cross entropy provided there might give you some additional intuition.  \n",
    "\n",
    "Side Note:  I do not advocate implementing this grading strategy in real life!\n",
    "\n",
    "Cross entropy is also called \"logistic Loss\" since it is the loss function used for logistic regression.  However, since it is also used for many other classification algorithms, calling it \"logistic loss\" (or \"log-loss\") is perhaps a bit anachronistic.\n",
    "\n",
    "Let's see how to implement this \"by hand\" using Pandas and NumPy.\n",
    "\n",
    "`ycs = pd.get_dummies(y_train).to_numpy()` will be an $n \\times C$ matrix whose $i^{th}$ row is the vector $y_i$.\n",
    "\n",
    "`pcs = LDA.predict_proba(X_train)` will be an $n \\times C$ matrix whose $i^{th}$ row is $P(y | x_i)$\n",
    "\n",
    "So we can calculate the cross entropy in a nice vectorized way as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467f94cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ycs = 1*pd.get_dummies(y_train).to_numpy()\n",
    "pcs = LDA.predict_proba(X_train[['petal_width', 'petal_length']])\n",
    "loss = - np.sum(ycs * np.log(pcs))\n",
    "print(f\"The cross entropy of this LDA classifier is{loss: .2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04686f3b",
   "metadata": {},
   "source": [
    "Let's break down the line `- np.sum(ycs * np.log(pcs))` a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7301a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing ycs and pcs side by side\n",
    "pd.DataFrame({'Class 0 Dummy': ycs[:,0], 'Class 1 Dummy': ycs[:,1], 'Class 2 dummy': ycs[:,2], 'Prob Class 0': np.round(pcs[:,0],2), 'Prob Class 1': np.round(pcs[:,1],2), 'Prob Class 2': np.round(pcs[:,2],2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ebe373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using ycs as a \"mask\" for pcs by multiplying componentwise (aka hadamard product)\n",
    "pd.DataFrame(ycs * pcs, columns=['Masked prob 0', 'Masked prob 1', 'Masked prob 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93869704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same, but take the negative log of the probs\n",
    "\n",
    "pd.DataFrame(-ycs * np.log(pcs), columns=['neg log prob 0', 'neg log prob 1', 'neg log prob 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9bba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the logs and summing.  Note that none of the predicted probabilities are *actually* zero, although they may be close, so log(0) isn't an issue.\n",
    "- np.sum(ycs * np.log(pcs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effa725f",
   "metadata": {},
   "source": [
    "##### In `sklearn`\n",
    "\n",
    "This can be done with `log_loss` in `sklearn`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html\">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6de1313",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import log_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9325b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## implement log_loss on the training set\n",
    "## first put in the true values\n",
    "## then the predicted probabilities\n",
    "## labels are the labels for the three classes [0,1,2]\n",
    "## normalize=False uses the log-loss formula we presented above. \n",
    "## normalize = True would divide this by the number of samples.\n",
    "log_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0515d309",
   "metadata": {},
   "source": [
    "##  A note on scoring rules and calibration\n",
    "\n",
    "Cross-entropy is an example of a [strictly proper scoring rule](https://en.wikipedia.org/wiki/Scoring_rule).  That means that the expected value of the score assigned to a distribution is minimized by the true distribution.\n",
    "\n",
    "A probabilistic model which has been trained by minimizing a strictly proper scoring rule is likely to be **well calibrated**:  the predicted probabilities should line up with the observed frequencies.  Since logistic regression uses cross entropy loss it tends to be well calibrated.\n",
    "\n",
    "The 538 post [\"Checking our work\"](https://projects.fivethirtyeight.com/checking-our-work/) looks at how well calibrated 538s predictions are.  It is a great piece for getting some intuitive understanding of why calibration is important!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22a3c6f",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2023.  Modified by Steven Gubkin 2024.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
