{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66af3534",
   "metadata": {},
   "source": [
    "# Scaling Data\n",
    "\n",
    "The problems in this notebook give you an opportunity to practice and extend the content covered in Lecture 4: Scaling Data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4b46e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645ba964",
   "metadata": {},
   "source": [
    "##### 1. Practice `StandardScaler`\n",
    "\n",
    "Load then standardize `X` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040da9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2039104)\n",
    "X = np.zeros((100, 20))\n",
    "\n",
    "for i in range(20):\n",
    "    multiplier = np.random.randint(0,10000)\n",
    "    constant = np.random.randint(-100,100)\n",
    "    X[:,i] = [constant + multiplier*np.random.randn(100), \n",
    "                 constant + multiplier*np.random.random(size=100),\n",
    "                 constant + multiplier*np.random.binomial(100,.2, 100)][np.random.randint(0,3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5010869",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0609ebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddd1a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb5a1e8",
   "metadata": {},
   "source": [
    "##### 2. `.mean_` and `.scale_`\n",
    "\n",
    "You can return the fitted mean of the `StandardScaler` with `.mean_` and you can find the fitted standard deviation with `.scale_`.\n",
    "\n",
    "Produce arrays of the means and standard deviations of `X` from 1. (Using `StandardScaler` not `numpy`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e02681",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be59e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d935e828",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9c6597",
   "metadata": {},
   "source": [
    "##### 3. Other scalers\n",
    "\n",
    "While we have used `StandardScaler` there are other scaler objects in `sklearn`. Here we will introduce some of the other scalers.\n",
    "\n",
    "- `MinMaxScaler`: This will scale your data so that the minimum value of the column is linearly scaled to `min` and the maximum value is linearly scaled to `max`, where `min` and `max` are inputs you control with `feature_range=(min, max)`. The default is such that your features get scaled to the interval $[0,1]$. <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler\">https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler</a>.\n",
    "- `MaxAbsScaler`: This will scale your data by dividing by the largest absolute value of the column, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html#sklearn.preprocessing.MaxAbsScaler\">https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html#sklearn.preprocessing.MaxAbsScaler</a>.\n",
    "- `RobustScaler`: This works in much the same way as `StandardScaler`, but it instead of the mean it subtracts the median and instead of the standard deviation it divides by the interquartile range. It is called \"robust\" because these metrics are more robust to outliers, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler\">https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler</a>.\n",
    "\n",
    "Choose one of these three scalers and scale the columns of `X` using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badf2cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a85daf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec87a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e3936b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c06081",
   "metadata": {},
   "source": [
    "##### 4. Scaling mixed columns\n",
    "\n",
    "When your data includes both quantitative and categorical variables scaling gets slightly more tricky. Demonstrate this by running the `X` below through `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900600a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((100,4))\n",
    "\n",
    "X[:,0] = np.random.randn(100)*10 + 89\n",
    "X[:,1] = np.random.binomial(1, .4, 100)\n",
    "X[:,2] = np.random.binomial(1, .6, 100)\n",
    "X[:,3] = np.random.binomial(1, .8, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0343a151",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First 10 rows of X\n",
    "## note that columns 1, 2, 3 are binary categorical variables\n",
    "## while column 0 is quantitative\n",
    "X[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc97afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d6113a",
   "metadata": {},
   "source": [
    "We should see that our three binary variables have been turned into nonsense columns. What we actually want is to scale column `0` but not columns `1,2,3`.\n",
    "\n",
    "You can do this, but it is slightly more complicated than what `sklearn`'s set scaler objects are capable of. Check out `Lectures/Cleaning/5. More Advanced Pipelines` to see one way to approach such an issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb00f0c",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2023.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erd≈ës Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bacf30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_fall_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
