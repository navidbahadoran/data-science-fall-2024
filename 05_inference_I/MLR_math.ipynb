{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's remind ourselves of the multiple linear regression setup.\n",
    "\n",
    "We presume a data generating process of the form\n",
    "\n",
    "$f(\\vec{x}) = \\vec{\\beta} \\cdot \\vec{x} + \\epsilon$\n",
    "\n",
    "where $\\vec{x}$ has been augmented with an initial $1$ and $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$.\n",
    "\n",
    "We have data $(\\vec{x}_i, y_i)$ for $i = 1, 2, 3, ..., n$ which we package into an $n \\times (p+1)$ design matrix $X$ and an outcome vector $\\vec{y}$.\n",
    "\n",
    "We give a very condensed derivation of the confidence intervals for the parameters here.  A fuller derivation is given in the math hour notes.\n",
    "\n",
    "Our parameter point estimates are given by the normal equations:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{\\beta} \n",
    "&= (X^\\top X)^{-1}X^\\top \\vec{y}\\\\\n",
    "&= (X^\\top X)^{-1}X^\\top (X \\beta + \\vec{\\epsilon})\\\\\n",
    "&= \\beta + (X^\\top X)^{-1}X^\\top \\vec{\\epsilon} \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Since $\\vec{\\epsilon} \\sim \\mathcal{N}(0, \\sigma^2 I)$ then $\\hat{\\beta}$ is also a multivariate normal distribution.  We can compute the covariance matrix as follows:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\operatorname{Cov}(\\hat{\\beta}) \n",
    "&= \\operatorname{Cov}((X^\\top X)^{-1}X^\\top \\vec{\\epsilon})\\\\\n",
    "&= (X^\\top X)^{-1}X^\\top\\operatorname{Cov}(\\vec{\\epsilon})((X^\\top X)^{-1}X^\\top)^\\top\\\\\n",
    "&= (X^\\top X)^{-1}X^\\top \\sigma^2 I X (X^\\top X)^{-1}\\\\\n",
    "&= \\sigma^2 (X^\\top X)^{-1}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Thus we have $\\hat{\\beta} \\sim \\mathcal{N}(\\beta, \\sigma^2 (X^\\top X)^{-1})$.  For each individual parameter $\\beta_i$ we then have\n",
    "\n",
    "$$\n",
    "\\frac{\\hat{\\beta}_{i}-\\beta_{i}}{\\sigma\\sqrt{(X^{T}X)^{-1}_{ii}}} \\sim N(0,1)\n",
    "$$\n",
    "\n",
    "Leaving the full details to math hour, we then replace the unknown $\\sigma^2$ with an unbiased estimate $s^2 = \\frac{1}{n-(p+1)} |\\vec{y} - \\hat{y}|^2$.  This leads to\n",
    "\n",
    "$$\n",
    "\\frac{\\hat{\\beta}_{i}-\\beta_{i}}{s\\sqrt{(X^{T}X)^{-1}_{ii}}} \\sim t_{n-(p+1)}\n",
    "$$\n",
    "\n",
    "so we obtain $1-\\alpha$ confidence intervals of\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_i \\pm t_{1-\\alpha/2, n- (p+1)} s \\sqrt{(X^{T}X)^{-1}_{ii}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from statsmodels.regression import linear_model\n",
    "\n",
    "# generating fake data with just 10 observations\n",
    "# statsmodels linear_model requires X to have an initial column of ones.\n",
    "nobs = 10\n",
    "X = np.random.randn(nobs,3)\n",
    "X[:,0] = 1\n",
    "y = np.dot(X, [3,5,-1]) + np.random.randn(nobs)\n",
    "\n",
    "# fitting an OLS linear model.\n",
    "# Notice that statsmodels has a different API than scikit-learn\n",
    "# When we instantiate the model we give it the data in the order (y,X)\n",
    "# We then fit the model.  Both of these conventions are \"backwards\" compared to scikit-learn.\n",
    "model = linear_model.OLS(y, X)\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.63743129,  3.74731654],\n",
       "       [ 4.77021259,  5.6820084 ],\n",
       "       [-1.32407739, -0.3552143 ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#These are the confidence intervals for the model parameters.  \n",
    "results.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "class LinearRegressionWithCI:\n",
    "    def __init__(self):\n",
    "        self.coefficients = None\n",
    "        self.cov_matrix = None\n",
    "        self.y_pred = None\n",
    "        self.residuals = None\n",
    "        self.mse = None\n",
    "        self.se = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Add intercept term\n",
    "        X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "        \n",
    "        # Implementing the normal equations\n",
    "        XtX_inv = np.linalg.inv(X.T @ X)\n",
    "        self.coefficients = XtX_inv @ X.T @ y\n",
    "        \n",
    "        self.y_pred = X @ self.coefficients\n",
    "        self.residuals = y - self.y_pred\n",
    "\n",
    "        # Degrees of freedom\n",
    "        df = self.residuals.shape[0] - self.coefficients.shape[0]\n",
    "\n",
    "        self.mse = np.sum(self.residuals**2) / df\n",
    "        self.cov_matrix = self.mse * XtX_inv\n",
    "        self.se = np.sqrt(np.diag(self.cov_matrix))\n",
    "\n",
    "    def confidence_intervals(self, alpha=0.05):\n",
    "        # Degrees of freedom\n",
    "        df = self.residuals.shape[0] - self.coefficients.shape[0]\n",
    "        \n",
    "        # t-value for the given confidence level\n",
    "        t_value = stats.t.ppf(1 - alpha/2, df)\n",
    "        \n",
    "        # Compute confidence intervals\n",
    "        intervals = []\n",
    "        for i in range(len(self.coefficients)):\n",
    "            lower_bound = self.coefficients[i] - t_value * self.se[i]\n",
    "            upper_bound = self.coefficients[i] + t_value * self.se[i]\n",
    "            intervals.append((lower_bound, upper_bound))\n",
    "        \n",
    "        return intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2.637431290479213, 3.747316535099524), (4.770212591057406, 5.682008399047496), (-1.3240773928350236, -0.35521429630848705)]\n",
      "\n",
      "\n",
      "[[ 2.63743129  3.74731654]\n",
      " [ 4.77021259  5.6820084 ]\n",
      " [-1.32407739 -0.3552143 ]]\n"
     ]
    }
   ],
   "source": [
    "reg = LinearRegressionWithCI()\n",
    "reg.fit(X[:,1:], y)\n",
    "print(reg.confidence_intervals())\n",
    "print(\"\\n\")\n",
    "print(results.conf_int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.957, 0.963)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Parameters for the simulation\n",
    "n_simulations = 1000\n",
    "n_samples = 100\n",
    "true_intercept = 2\n",
    "true_slope = 3\n",
    "\n",
    "# Storage for results\n",
    "intercept_within_ci = 0\n",
    "slope_within_ci = 0\n",
    "\n",
    "# Run the simulations\n",
    "for _ in range(n_simulations):\n",
    "    # Simulate data\n",
    "    x = np.random.normal(size=n_samples)\n",
    "    eps = np.random.normal(size=n_samples)\n",
    "    y = true_intercept + true_slope * x + eps\n",
    "    \n",
    "    # Fit the model\n",
    "    x_with_const = sm.add_constant(x)  # Adds intercept term to the model\n",
    "    model = sm.OLS(y, x_with_const)\n",
    "    results = model.fit()\n",
    "    \n",
    "    # Get the confidence intervals\n",
    "    ci = results.conf_int(alpha=0.05)\n",
    "    \n",
    "    # Check if true parameters are within the CIs\n",
    "    if ci[0, 0] <= true_intercept <= ci[0, 1]:\n",
    "        intercept_within_ci += 1\n",
    "    if ci[1, 0] <= true_slope <= ci[1, 1]:\n",
    "        slope_within_ci += 1\n",
    "\n",
    "# Calculate the proportion of times the true parameters were within the confidence intervals\n",
    "intercept_coverage = intercept_within_ci / n_simulations\n",
    "slope_coverage = slope_within_ci / n_simulations\n",
    "\n",
    "intercept_coverage, slope_coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
