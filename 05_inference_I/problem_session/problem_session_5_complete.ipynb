{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Session 5\n",
    "\n",
    "The problems in this notebook will cover the content covered in our Inference I lectures including:\n",
    "- Hypothesis Testing\n",
    "- Confidence Intervals\n",
    "- Linear Regression Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. The dangers of early peeking in A/B testing\n",
    "\n",
    "One especially common use of hypothesis testing in data science is \"A/B testing\".\n",
    "\n",
    "Many companies which prioritize data informed decision making have very mature A/B testing platforms to trial changes before adoption.  Here is an example of an A/B test which comes from the book \"Trustworthy Online Controlled Experiments\" by Kohavi, Tang and Xu:\n",
    "\n",
    "Someone at your company proposes implementing a coupon code system.  To rapidly get some idea of the potential impacts even *before* implementing the complete system you decide to implement the following A/B test:  for a period of two weeks you will show half of your customers your standard checkout page (the \"control group\"), and you will show the other half a new checkout page which has a coupon code box (the \"treatment group\"). Since there are no coupon codes in existence yet, putting anything in the box will simply display \"invalid code\" and otherwise do nothing.\n",
    "\n",
    "You will monitor how customers interact with the coupon code box (how many people click on it, enter anything into it, enter one or more attempted codes, etc), how long they stay on the checkout page in the control and treatment group, what fraction of customers who make it to the checkout page who actually complete their purchase, and the revenue per customer who made it to the checkout page.\n",
    "\n",
    "In this example, the mere presence of a coupon code box significantly reduced revenue per customer with an effect size large enough to scuttle the project!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common (but ill advised) for companies to **continuously** monitor such experiments and stop early when a significant result is obtained in either direction.  The reasoning is that we would not want to continue a disastrous experiment for the full planned time (e.g.  hardly anyone checks out after being presented with the coupon code box), and we would likewise not want to miss out on the benefits by **not** implementing a positive experiment as soon as possible.\n",
    "\n",
    "In this exercise we will see why early stopping is such a bad idea through simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a)\n",
    "\n",
    "Finish the definition of the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simulate_data(control_mean, treatment_mean, scale, size):\n",
    "    '''\n",
    "    Draws from two normal distributions with the same scale and different means\n",
    "\n",
    "    Args:\n",
    "        control_mean:  the mean of the control group\n",
    "        treatment_mean: the mean of the treatment group\n",
    "        scale: the common standard deviation of both groups\n",
    "        size: the shape of both outputs\n",
    "    \n",
    "    returns:\n",
    "        The tuple (control_data, treatment_data)        \n",
    "    '''\n",
    "    control_data = np.random.normal(loc = control_mean, scale = scale, size = size)\n",
    "    treatment_data = np.random.normal(loc = treatment_mean, scale = scale, size = size)\n",
    "    return (control_data, treatment_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b)\n",
    "\n",
    "Read the following documentation:\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html\n",
    "\n",
    "Use the `ttest_ind` to finish writing the following two functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.weightstats import ttest_ind\n",
    "\n",
    "def no_early_peeking_results(control_data, treatment_data):\n",
    "    '''\n",
    "    Returns the p-value of the t-test comparing the two group means.\n",
    "    '''\n",
    "    p_value = ttest_ind(control_data, treatment_data)[1]\n",
    "    return p_value\n",
    "\n",
    "assert(no_early_peeking_results([1,2,3], [4,5,6]) == 0.021311641128756727)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_peeking_results(control_data, treatment_data, alpha):\n",
    "    '''\n",
    "    Runs a t-test on each initial segment of the control and treatment data.\n",
    "\n",
    "    Args:\n",
    "        alpha: the threshold for significance.\n",
    "        \n",
    "    Returns: \n",
    "        (p_value, nobs)\n",
    "        p_value: the p-value of the first significant such test\n",
    "        nobs: the number of observations in that test\n",
    "    \n",
    "    Example:  \n",
    "        early_peeking_results([1,2,2,2,2,2], [4,5,5,5,5,5], 0.05)\n",
    "        Should run a t-test on \n",
    "            [1,2], [4,5]\n",
    "            [1,2,2], [4,5,5]\n",
    "            [1,2,2,2], [4,5,5,5]\n",
    "            etc\n",
    "        when a significant p-value is found it will output\n",
    "        that p-value and the length of the control group for that test.\n",
    "    '''\n",
    "    p_value = 1\n",
    "    nobs = 2\n",
    "    while p_value > alpha and nobs < len(control_data):\n",
    "        p_value = ttest_ind(control_data[:nobs], treatment_data[:nobs])[1]\n",
    "        nobs += 1\n",
    "    return p_value, nobs\n",
    "\n",
    "assert(early_peeking_results([1,2,2,2,2,2], [4,5,5,5,5,5], 0.05)[0] == 0.0031255892524457277)\n",
    "assert(early_peeking_results([1,2,2,2,2,2], [4,5,5,5,5,5], 0.05)[1] == 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c)\n",
    "\n",
    "We will now see the impact of early peeking on the false positive rate.\n",
    "\n",
    "By setting both the control and training mean to 0, and $\\alpha = 0.05$ for the threshold for significance, we should expect to see a false positive rate of roughly $0.05$.  We will see that early peeking wildly inflates the false positive rate!\n",
    "\n",
    "This is bad news for our company, because we will be mislead into thinking that our treatment has a significant effect (in either direction) when there really is no effect at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparing_procedures(control_mean = 0, \n",
    "                            treatment_mean = 0, \n",
    "                            scale = 1, \n",
    "                            size = 1000, \n",
    "                            num_simulations = 100, \n",
    "                            alpha = 0.05):\n",
    "    '''\n",
    "    Simulates data from both control and treatment groups.\n",
    "    Returns the following self explanatory variables:\n",
    "        no_early_peeking_false_positives of type int\n",
    "        early_peeking_false_positives of type int\n",
    "        early_peeking_nobs of type list(int)\n",
    "    '''                \n",
    "    no_early_peeking_false_positives = 0\n",
    "    early_peeking_false_positives = 0\n",
    "    early_peeking_nobs = []\n",
    "\n",
    "    for i in range(100):\n",
    "        control_data, treatment_data = simulate_data(control_mean,treatment_mean,scale,size)\n",
    "        no_early_peeking_p_value = no_early_peeking_results(control_data, treatment_data)\n",
    "        early_peeking_p_value, nobs = early_peeking_results(control_data, treatment_data, alpha)\n",
    "        if no_early_peeking_p_value < 0.05:\n",
    "            no_early_peeking_false_positives += 1\n",
    "        if early_peeking_p_value < 0.05:\n",
    "            early_peeking_false_positives += 1\n",
    "        early_peeking_nobs.append(nobs)\n",
    "\n",
    "    early_peeking_nobs = np.array(early_peeking_nobs)\n",
    "\n",
    "    return (no_early_peeking_false_positives, early_peeking_false_positives, early_peeking_nobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of false positives with no early peeking is 4 out of 100\n",
      "The number of false positives with early peeking is 58 out of 100\n",
      "The number of observations to reach those false positives (out of 1000) were \n",
      " [  4  86  87   4   3   9 704   4  31 166  81   3  41  11 901   3   9 550\n",
      " 764   5   4  15  31   5  12   7 953 190 388   4   4   8  62  94  63  12\n",
      " 922   7   4  23   6 262  80   6   3  38  50 249  68  24  55 233 138   4\n",
      "   6   9 220   3]\n"
     ]
    }
   ],
   "source": [
    "num_simulations = 100\n",
    "size = 1000\n",
    "(no_early_peeking_false_positives, early_peeking_false_positives, early_peeking_nobs) = comparing_procedures(num_simulations=num_simulations, size = size)\n",
    "print(f\"The number of false positives with no early peeking is {no_early_peeking_false_positives} out of {num_simulations}\")\n",
    "print(f\"The number of false positives with early peeking is {early_peeking_false_positives} out of {num_simulations}\")\n",
    "print(f\"The number of observations to reach those false positives (out of {size}) were \\n {early_peeking_nobs[early_peeking_nobs != 1000]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Confidence Intervals and Linear Regression Coefficients\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_fall_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
