{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd2166b8",
   "metadata": {},
   "source": [
    "# $k$-Nearest Neighbors\n",
    "\n",
    "We will introduce our first classification algorithm, $k$-nearest neighbors ($k$-NN). \n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Introduce $k$-NN classification,\n",
    "- Discuss our first classification performance metric and\n",
    "- See the `iris` data set for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c65a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06a44b9",
   "metadata": {},
   "source": [
    "## The algorithm\n",
    "\n",
    "The way that $k$-NN makes predictions from the training set is pretty straightforward.\n",
    "- You input a point you would like to predict on, $X^*$,\n",
    "- It finds the $k$ closest points to $X^*$ in the training set, these are called $X^*$'s nearest neighbors,\n",
    "- The categories of each of the nearest neighbors are tabulated and\n",
    "- The category that receives the most <i>votes</i> is what is predicted for $X^*$,\n",
    "    - If there is a tie between two or more categories, the prediction with the smaller index is chosen.  Note that this does bias the algorithm somewhat in favor of classes with smaller indexes.\n",
    "    \n",
    "As always this may be easier to understand with pictures. Suppose we have training data with $2$ features that can be one of two classes, one represented by a red circle the other represented by a green triangle. In these examples I have chosen $k=4$.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"lecture_8_assets/knn1.png\" width=\"60%\"></img>\n",
    "\n",
    "Here $k$-NN would predict that the data point represented by the black X is a red circle.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"lecture_8_assets/knn2.png\" width=\"60%\"></img>\n",
    "\n",
    "Here $k$-NN would predict that the data point represented by the black X is a green triangle.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"lecture_8_assets/knn3.png\" width=\"60%\"></img>\n",
    "\n",
    "Here $k$-NN would randomly choose between a red circle and a green triangle for the data point represented by the black X.\n",
    "\n",
    "<i>Note, that while we seemingly used Euclidean distance in these example, we can in principle use any distance metric we would like. Also while we gave each neighbor an equally weighted vote, we could weight the votes. One way votes are weighted is the inverse of the distance to our data point of interest.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bda1c8",
   "metadata": {},
   "source": [
    "## $k$-NN in `sklearn`\n",
    "\n",
    "### The iris data set\n",
    "\n",
    "We will demonstrate how to implement $k$-NN on a famous data set whose description can be found here <a href=\"https://archive.ics.uci.edu/ml/datasets/Iris\">https://archive.ics.uci.edu/ml/datasets/Iris</a>. This is a very popular data set for testing classification algorithms.\n",
    "\n",
    "Each observation represents an iris (a type of flower) and gives it's measurements including:\n",
    "- `sepal_length`: the length of the iris's sepal in cm.\n",
    "- `sepal_width`: the width of the iris's sepal in cm.\n",
    "- `petal_length`: the length of the iris's petal in cm.\n",
    "- `petal_width`: the width of the iris's petal in cm.\n",
    "- `iris_class`: the class of the iris, can be:\n",
    "    - `0` meaning it is a setosa iris\n",
    "    - `1` meaning it is a versicolor iris\n",
    "    - `2` meaning it is a virginica iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390511da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## to get the iris data\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "## Load the data\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(iris['data'],columns = ['sepal_length','sepal_width','petal_length','petal_width'])\n",
    "iris_df['iris_class'] = iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a113f4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9535e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579e761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making a statified train test split\n",
    "iris_train, iris_test = train_test_split(iris_df, \n",
    "                                            random_state=431,\n",
    "                                            shuffle=True,\n",
    "                                            test_size=.2,\n",
    "                                            stratify=iris_df['iris_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b12cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992fb7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the training data\n",
    "## sepal_width against sepal_length\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.scatter(iris_train.loc[iris_train.iris_class==0].sepal_width,\n",
    "            iris_train.loc[iris_train.iris_class==0].sepal_length,\n",
    "            c='blue',\n",
    "            s=60,\n",
    "            label=\"0\")\n",
    "\n",
    "plt.scatter(iris_train.loc[iris_train.iris_class==1].sepal_width,\n",
    "            iris_train.loc[iris_train.iris_class==1].sepal_length,\n",
    "            c='orange',\n",
    "            s=60,\n",
    "            marker='v',\n",
    "            label=\"1\")\n",
    "\n",
    "plt.scatter(iris_train.loc[iris_train.iris_class==2].sepal_width,\n",
    "            iris_train.loc[iris_train.iris_class==2].sepal_length,\n",
    "            c='green',\n",
    "            s=60,\n",
    "            marker='x',\n",
    "            label=\"2\")\n",
    "\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.xlabel(\"Sepal Width\", fontsize=12)\n",
    "plt.ylabel(\"Sepal Length\", fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cff435",
   "metadata": {},
   "source": [
    "$k$-NN can be implemented in `sklearn` with `KNeighborsClassifier`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\">https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html</a>.\n",
    "\n",
    "We will now build a $k$-NN classifier using this model object, with $k=5$. We will not implement cross-validation because I am just demonstrating how to \"fit\" a $k$-NN model with `sklearn`. (Note fit is in quotation marks because this algorithm actually has no fitting step!).\n",
    "\n",
    "Note that we should generally standardize our features before using $k$-NN so that the classification is not sensitive to the scale of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f320a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import knn, standard scaler, and pipeline here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1b4f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "\n",
    "## make the model object\n",
    "knn_pipe = \n",
    "\n",
    "## \"fit\" the model object\n",
    "\n",
    "## predict on the training set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4920dc93",
   "metadata": {},
   "source": [
    "##### How to measure classification performance?\n",
    "\n",
    "There are many ways! Perhaps the most common, or default approach is to use <i>accuracy</i>. Accuracy measures the proportion of all predictions made that are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc0437e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We define it by hand here\n",
    "## but you can also use accuracy_score from sklearn\n",
    "## https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
    "def accuracy(true, predicted):\n",
    "    return np.sum(true==predicted)/len(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5900534",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the training accuracy for our model here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc49a40",
   "metadata": {},
   "source": [
    "#### Prediction probabilities\n",
    "\n",
    "Before finishing this notebook we will see how to get classification probabilities with `sklearn`. For many applications it is more useful to have the probability that a certain observation is a certain class, rather than a predicted class itself. `sklearn` classification models have a `predict_proba` method that gives this. For unweighted $k$-NN this gives the fraction of the observations nearest neighbors that are of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b0213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## show predict_proba\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908dbefc",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2023.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_fall_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
