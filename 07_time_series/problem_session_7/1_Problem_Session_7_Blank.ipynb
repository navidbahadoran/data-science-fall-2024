{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcf6f50b",
   "metadata": {},
   "source": [
    "# Problem Session 7\n",
    "## Forecasting The Bachelorette and Pumpkin Spice II\n",
    "\n",
    "In the second of two time series based problem sessions you build upon your work in `Problem Session 6`. In particular you will look to build the best forecast you can for the Bachelorette IMDB ratings. Afterwards you will practice using a SARIMA model with the pumpkin spice Google trends data.\n",
    "\n",
    "The problems in this notebook will cover the content covered in our `Time Series Forecasting` lectures including:\n",
    "- `Averaging and Smoothing`,\n",
    "- `Stationarity and Autocorrelation` and\n",
    "- `SARIMA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cf50e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "from datetime import datetime\n",
    "\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bba8849",
   "metadata": {},
   "source": [
    "#### 1. The Bachelorette\n",
    "\n",
    "##### a.\n",
    "Here we:\n",
    "\n",
    "- Reload the Bachelorette IMDB data stored in `the_bachelorette.csv` in the `data` folder. \n",
    "- Look at the first five rows.\n",
    "- Make a train test split setting aside the last three episodes as a test set.\n",
    "- Visualize the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7447489",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv = pd.read_csv(filepath_or_buffer=\"../../data/the_bachelorette.csv\")\n",
    "tv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f04d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_train = # your code here\n",
    "tv_test = # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b978c1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,5))\n",
    "\n",
    "for season in range(1, tv_train.season.max()+1):\n",
    "    plt.plot(tv_train.loc[tv_train.season==season].episode_number,\n",
    "                tv_train.loc[tv_train.season==season].imdb_rating,\n",
    "                '-o')\n",
    "\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "plt.xlabel(\"Episode Number\", fontsize=12)\n",
    "plt.ylabel(\"IMDB Rating\", fontsize=12)\n",
    "\n",
    "plt.title(\"The Bachelorette IMDB Ratings\\nColored by Season\", fontsize=14)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6363a793",
   "metadata": {},
   "source": [
    "Some notes on the dataset\n",
    "\n",
    "* These are *subjective* assessments of episode quality which are made by self-selected raters.  This is far from a random sample.\n",
    "* A lot of the variability will be due to things which are difficult to predict, such as popularity of particular contestants.\n",
    "\n",
    "For these reasons we cannot expect to generate a forecast with much predictive ability.  We shouldn't really *expect* to be able to beat simple baselines like the naive baseline, rolling average, or random walk with drift.  The use of more \"advanced\" models is probably ill advised.  We are implementing more advanced models here to practice using them, not because it would be a good idea to actually trust these models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8721870c",
   "metadata": {},
   "source": [
    "##### b.\n",
    "\n",
    "Here is a refresher on the columns of this data.\n",
    "\n",
    "- `episode_number` is the number of the episode with respect to the entire series run,\n",
    "- `title` is the title of the episode,\n",
    "- `season` is the number of the season in which the episode aired,\n",
    "- `season_episode_number` is the number of the episode with respect to the season in which it aired,\n",
    "- `imdb_rating` is the average rating of the episode among IMDB's users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2b0e41",
   "metadata": {},
   "source": [
    "##### c.\n",
    "\n",
    "The first model you will fit is a rolling average model. In this problem you will be tuning the moving average window size, $q$, to find the value that minimizes the Mean Absolute Scaled Error (defined below).\n",
    "\n",
    "Fill in the missing chunks of code to perform hyperparameter tuning for $q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5815e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes 3 numpy arrays as inputs.\n",
    "# It returns the \n",
    "# mean absolute error of the forecast on the test set \n",
    "# relative to the\n",
    "# mean absolute error of the naive forecast on the training set over an equivalent horizon.\n",
    "# \n",
    "# If MASE > 1 then your forecast performs worse out of sample than naive forecast does in sample.\n",
    "# If MASE < 1 then your forecast performs better out of sample than naive forecast does in sample.\n",
    "\n",
    "def mase(y_train, y_test, y_preds):\n",
    "    n = len(y_train)\n",
    "    m = len(y_test)\n",
    "    denom = 0\n",
    "    for i in range(n-m):\n",
    "        denom += np.abs(y_train[i+1:i+m+1] - y_train[i]*np.ones(m)).mean()\n",
    "    denom = denom / (n-m)\n",
    "    num = np.abs(y_test - y_preds).mean()\n",
    "    return num/denom\n",
    "\n",
    "# Example calculation\n",
    "\n",
    "print('MASE = ', mase(y_train = np.array([1,2,3,4,5]), y_test = np.array([6,7]), y_preds = np.array([3, 3])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6db5b8",
   "metadata": {},
   "source": [
    "This would be computed by hand as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\textrm{Mean absolute prediction error out of sample}}{\\textrm{Mean absolute naive forecast prediction error in sample}} = \\frac{\\frac{1}{2} \\left( |6-3| + |7-3|\\right)}{\\frac{1}{3} \\left[ \\frac{1}{2}\\left(|2-1| + |3-1| \\right) + \\frac{1}{2}\\left(|3-2| + |4-2| \\right) + \\frac{1}{2}\\left(|4-3| + |5-3| \\right)  \\right]} = 2.333\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bf8168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22271dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = TimeSeriesSplit(10, test_size=3)\n",
    "\n",
    "start = 2\n",
    "end = 31\n",
    "ra_mase = np.zeros((10, len(range(start, end))))\n",
    "\n",
    "\n",
    "i = 0\n",
    "for train_index, test_index in cv.split(tv_train):\n",
    "    tv_tt = # your code here\n",
    "    tv_ho = # your code here\n",
    "    \n",
    "    j = 0\n",
    "    for q in range(start, end):\n",
    "        pred = # your code here\n",
    "        \n",
    "        ra_mase[i,j] = # your code here\n",
    "        j = j + 1\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c628f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.scatter(range(start,end), np.mean(ra_mase, axis=0))\n",
    "\n",
    "plt.xlabel(\"Window Size\", fontsize=12)\n",
    "plt.ylabel(\"Average MASE\", fontsize=12)\n",
    "\n",
    "plt.xticks(range(start, end, 3), fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2d9b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The window size that minimized the avg. cv MASE\",\n",
    "      \"was q =\", \n",
    "      range(start,end)[np.argmin(np.mean(ra_mase, axis=0))],\n",
    "      \"\\b.\",\n",
    "      \"It had a mean cv MASE of\", \n",
    "      np.round(np.min(np.mean(ra_mase, axis=0)), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513947e5",
   "metadata": {},
   "source": [
    "##### d.\n",
    "\n",
    "The second model you will try is an exponential smoothing model.\n",
    "\n",
    "Because these data exhibit a trend but not seasonality we will fit a double exponential smoothing model. For this we will want to find the best $\\alpha$ (The smoothing on the time series) and $\\beta$ (the smoothing on the trend component).\n",
    "\n",
    "Fill in the missing code chunks below to perform a grid search for the values of $\\alpha$ and $\\beta$ that minimize the average CV RMSE. (Note that a grid search is what we call it when you perform hyperparameter tuning with a grid of possible hyperparameter values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1431430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.holtwinters import Holt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8761d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_mase = np.zeros((10, len(np.arange(0, 0.2, .01)), len(np.arange(0, 0.2, .01))))\n",
    "\n",
    "i = 0\n",
    "for train_index, test_index in cv.split(tv_train):\n",
    "    tv_tt = # your code here\n",
    "    tv_ho = # your code here\n",
    "    \n",
    "    j = 0\n",
    "    for alpha in np.arange(0, 0.2, .01):\n",
    "        k = 0\n",
    "        for beta in np.arange(0, 0.2, .01):\n",
    "            exp_smooth = # your code here\n",
    "\n",
    "            exp_mase[i,j,k] = # your code here\n",
    "            k = k + 1\n",
    "        j = j + 1\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9ff6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This gives us the indices of the smallest\n",
    "## avg cv rmse\n",
    "exp_ind = np.unravel_index(np.argmin(np.mean(exp_mase, axis=0), axis=None), \n",
    "                           np.mean(exp_mase, axis=0).shape)\n",
    "np.unravel_index(np.argmin(np.mean(exp_mase, axis=0), axis=None), \n",
    "                 np.mean(exp_mase, axis=0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a3c5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The alpha and beta values that give a double exponential\",\n",
    "         \"smoothing model with lowest avg cv rmse are\",\n",
    "         \"alpha = \", np.arange(0, 0.2, .01)[exp_ind[0]],\n",
    "         \"and beta = \", np.arange(0, 0.2, .01)[exp_ind[1]])\n",
    "\n",
    "print(\"This model had an avg cv rmse of\",\n",
    "         np.round(np.mean(exp_mase, axis=0)[exp_ind],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3460aa8e",
   "metadata": {},
   "source": [
    "##### e.\n",
    "\n",
    "The final model you will try is an ARIMA model. \n",
    "\n",
    "First let's check the stationarity assumption for this time series. Make an autocorrelation plot of the training data. If you find that the ACF plot indicates that the time series is non-stationary, plot the ACF of the time series' first differences. Do these appear to be stationary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30094ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bf6854",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(7,5))\n",
    "\n",
    "sm.graphics.tsa.plot_acf(tv_train.imdb_rating.values,\n",
    "                            lags = 40,\n",
    "                            ax = ax)\n",
    "\n",
    "plt.title('The Bachelorette IMDB rating ACF', fontsize=14)\n",
    "plt.ylabel(\"Autocorrelation\", fontsize=12)\n",
    "plt.xlabel(\"Lag\", fontsize=12)\n",
    "\n",
    "plt.ylim(-1.1,1.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25154d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8,5))\n",
    "\n",
    "# plot ACF of first differenced data here\n",
    "\n",
    "\n",
    "plt.title('The Bachelorette First Differences ACF', fontsize=14)\n",
    "plt.ylabel(\"Autocorrelation\", fontsize=12)\n",
    "plt.xlabel(\"Lag\", fontsize=12)\n",
    "\n",
    "plt.ylim(-1.1,1.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175ae751",
   "metadata": {},
   "source": [
    "##### f.\n",
    "\n",
    "From what we saw above what we should set our $d$ value in the ARIMA model?. Set $d$ to this value and then perform hyperparameter tuning to find the values of $p$ and $q$ that give us the lowest mean CV RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62854f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985d0404",
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_mase = np.zeros((10, 4, 4))\n",
    "\n",
    "i = 0\n",
    "for train_index, test_index in cv.split(tv_train):\n",
    "\n",
    "    tv_tt = tv_train.iloc[train_index]\n",
    "    tv_ho = tv_train.iloc[test_index]\n",
    "    \n",
    "    j = 0\n",
    "    for p in range(4):\n",
    "        k = 0\n",
    "        for q in range(4):\n",
    "            arima = # your code here\n",
    "            \n",
    "            arima_mase[i,j,k] = # your code here\n",
    "            k = k +1\n",
    "        j = j + 1\n",
    "    i = i +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051d7343",
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_ind = np.unravel_index(np.argmin(np.mean(arima_mase, axis=0), axis=None), \n",
    "                             np.mean(arima_mase, axis=0).shape)\n",
    "np.unravel_index(np.argmin(np.mean(arima_mase, axis=0), axis=None), \n",
    "                 np.mean(arima_mase, axis=0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aed555e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The parameters that give an ARIMA model\",\n",
    "         \"with lowest avg cv mase are\",\n",
    "         \"(p,d,q) = ( %s, %s, %s )\" %(range(4)[arima_ind[0]], 1, range(4)[arima_ind[1]]))\n",
    "\n",
    "print(\"This model had an avg cv mase of\",\n",
    "         np.round(np.mean(arima_mase, axis=0)[arima_ind],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f925b1d",
   "metadata": {},
   "source": [
    "As an alternative to cross validation MASE, another common way to select ARIMA parameters is by minimizing the [Akaike Information Criterion (AIC)](https://en.wikipedia.org/wiki/Akaike_information_criterion) on the training set.  Let's see whether this yields the same hyperparameters as we obtained above.  There is a handy object called `auto_arima` from `pmdarima` which can do automatic order selection in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc682bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima import auto_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee07ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_arima(tv_train.imdb_rating.values, trace=True, max_p=4, max_q=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df7cf44",
   "metadata": {},
   "source": [
    "We obtained the same parameters!  Let's also check the model summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c70b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "arima = ARIMA(tv_train.imdb_rating.values,order = # your code here).fit()\n",
    "arima.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6081c1e",
   "metadata": {},
   "source": [
    "##### g.\n",
    "\n",
    "Plot your forecast (along with the training and test data) using the model with the lowest CV MASE.\n",
    "\n",
    "What is the MASE on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0859b807",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mase = # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32456751",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "plt.scatter(tv_train.episode_number,\n",
    "               tv_train.imdb_rating,\n",
    "               alpha=.5,\n",
    "               label=\"Training Points\")\n",
    "\n",
    "plt.scatter(tv_test.episode_number,\n",
    "               tv_test.imdb_rating,\n",
    "               alpha=.5,\n",
    "               c = 'red',\n",
    "               marker = 'v',\n",
    "               label=\"Test Points\")\n",
    "\n",
    "plt.plot(tv_train.episode_number,\n",
    "            # your code here,\n",
    "            'k-',\n",
    "            linewidth = 2,\n",
    "            label=\"Fitted Values\")\n",
    "\n",
    "plt.plot(tv_test.episode_number,\n",
    "            # your code here,\n",
    "            'r--',\n",
    "            linewidth=2,\n",
    "            label=\"Forecast\")\n",
    "\n",
    "plt.legend(fontsize=12, loc=3)\n",
    "\n",
    "plt.title(\"Test Set MASE = \" + str(np.round(test_mase,3)),\n",
    "             fontsize=14)\n",
    "\n",
    "plt.xlabel(\"Episode Number\", fontsize=12)\n",
    "plt.ylabel(\"IMDB Rating\", fontsize=12)\n",
    "\n",
    "\n",
    "plt.ylim(3,8.5)\n",
    "plt.xlim(180,220)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc55f33",
   "metadata": {},
   "source": [
    "#### 2. Pumpkin spice SARIMA\n",
    "\n",
    "We won't have enough time in this problem session to do more extensive model building and comparison for the Pumpkin Spice data forecasting problem.  We will just use this as opportunity to practice fitting SARIMA models.\n",
    "\n",
    "Load the data stored in `pumpkin_spice.csv` in the `Data` folder then look at the first five rows. Then make a train test split setting aside all observations on or after January 1, 2022 aside as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d3bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pumpkin = pd.read_csv(\"../../data/pumpkin_spice.csv\",\n",
    "                         parse_dates = [\"Month\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150d93d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_train = pumpkin.loc[pumpkin.Month < datetime(2022, 1, 1)].copy()\n",
    "p_test = pumpkin.drop(p_train.index).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f021e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,5))\n",
    "\n",
    "plt.plot(pumpkin.Month,\n",
    "            pumpkin.interest_level,\n",
    "            '-o')\n",
    "\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.ylabel(\"Google Interest Level\", fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b0ac66",
   "metadata": {},
   "source": [
    "##### b.\n",
    "\n",
    "In lecture we talked about first differencing non-stationary time series exhibiting a trend to create a, seemingly, stationary time series.\n",
    "\n",
    "This can also be done for seasonal data. Suppose that we suspect a time series, $\\left\\lbrace y_t \\right\\rbrace$ exhibits seasonality where a season lasts $m$ time steps. Then the first seasonal differenced time series is:\n",
    "\n",
    "$$\n",
    "\\nabla_s y_t = y_t - y_{t-m}.\n",
    "$$\n",
    "\n",
    "Plot the autocorrelation of the training set, then perform first seasonal differencing on these data and plot the autocorrelation of the first seasonal differenced series.\n",
    "\n",
    "Does the differenced series appear less likely to violate stationarity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1add89d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.tsa.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512c2817",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1, figsize=(12,5))\n",
    "\n",
    "sm.graphics.plot_acf(p_train.interest_level.values,\n",
    "                        lags = 36,\n",
    "                        ax = ax)\n",
    "\n",
    "plt.title('Autocorrelation of the original data')\n",
    "\n",
    "plt.ylim([-1.1,1.1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a95cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1, figsize=(12,5))\n",
    "\n",
    "# plot first seasonal differenced data here\n",
    "\n",
    "plt.title('Autocorrelation of first seasonal differenced data')\n",
    "plt.ylim([-1.1,1.1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fad46e7",
   "metadata": {},
   "source": [
    "##### c.\n",
    "\n",
    "Recall for an $\\text{ARIMA}$ model you needed parameters $p$, $d$ and $q$. For a $\\text{SARIMA}$ model you need parameters $P$, $D$, $Q$ and $m$ as well:\n",
    "\n",
    "- $P$ is the order of the seasonal autoregressive portion of the model,\n",
    "- $Q$ is the order of the seasonal moving average portion of the model,\n",
    "- $D$ is the order of the seasonal differencing and\n",
    "- $m$ is the number of time steps that take place in a single period.\n",
    "\n",
    "Use `auto_arima` with $D=1$ and $m=12$ and determine the AIC minimizing values of $p,d,q,P,Q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9debc088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use auto_arima to identify the best hyperparameters here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a6e2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit an ARIMA model using those hyperparameters\n",
    "\n",
    "sarima = # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7049e408",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "\n",
    "plt.plot(p_train.Month,\n",
    "            p_train.interest_level,\n",
    "            'b-o',\n",
    "            label='Training Data')\n",
    "\n",
    "\n",
    "plt.plot(p_test.Month,\n",
    "            p_test.interest_level,\n",
    "            color = 'orange',\n",
    "            marker = 'o',\n",
    "            label='Test Data')\n",
    "\n",
    "plt.plot(p_train.Month,\n",
    "            sarima.fittedvalues,\n",
    "            'r',\n",
    "            label='Fitted Values')\n",
    "\n",
    "plt.plot(p_test.Month,\n",
    "               sarima.forecast(len(p_test)),\n",
    "               '--r',\n",
    "               label=\"Forecast\")\n",
    "\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel(\"Date\", fontsize=14)\n",
    "plt.ylabel(\"Interest Level\", fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168c713f",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2023. Modified by Steven Gubkin 2024.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
